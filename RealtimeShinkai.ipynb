{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5f32994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abstu\\AppData\\Local\\Temp\\ipykernel_8580\\2036234583.py:73: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  input_image = input_image.resize((h, w), Image.BICUBIC)\n",
      "C:\\Users\\abstu\\AppData\\Local\\Temp\\ipykernel_8580\\2036234583.py:80: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  input_image = Variable(input_image, volatile=True).float()\n",
      "f:\\Work\\Anaconda\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "from network.Transformer import Transformer\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--input_dir', default = 'test_img')\n",
    "#parser.add_argument('--load_size', default = 450)\n",
    "#parser.add_argument('--model_path', default = './pretrained_model')\n",
    "#parser.add_argument('--style', default = 'Hayao')\n",
    "#parser.add_argument('--output_dir', default = 'test_output1')\n",
    "#parser.add_argument('--gpu', type=int, default = -1)\n",
    "\n",
    "#opt = parser.parse_args()\n",
    "\n",
    "model_path = 'F:\\Work\\ML Projects\\CartoonGAN-Test-Pytorch-Torch\\Shinkai_net_G_float.pth'\n",
    "\n",
    "\n",
    "Shinkai_net_G_float = torch.load(model_path)\n",
    "valid_ext = ['.jpg', '.png']\n",
    "\n",
    "if not os.path.exists(\"test_output1\"): os.mkdir(\"test_output1\")\n",
    "\n",
    "# load pretrained model\n",
    "model = Transformer()\n",
    "model.load_state_dict(Shinkai_net_G_float)\n",
    "model.eval()\n",
    "\n",
    "print('CPU mode')\n",
    "model.float()\n",
    "\n",
    "\n",
    "# Initialize the video capture device\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize a frame counter\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    # Capture the current frame\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # Check if the video capture was successful\n",
    "    if not ret:\n",
    "        print(\"Failed to capture video\")\n",
    "        break\n",
    "\n",
    "   \n",
    "    # Save the frame as an image file\n",
    "    filename = f\"frame{frame_count}.jpg\"\n",
    "    cv2.imwrite(filename, frame)\n",
    "    with torch.no_grad():\n",
    "    # load image\n",
    "        input_image = Image.open(filename).convert(\"RGB\")\n",
    "    \n",
    "    # resize image, keep aspect ratio\n",
    "        h = input_image.size[0]\n",
    "        w = input_image.size[1]\n",
    "        ratio = h *1.0 / w\n",
    "        if ratio > 1:\n",
    "            h = 450\n",
    "            w = int(h*1.0/ratio)\n",
    "        else:\n",
    "            w = 450\n",
    "            h = int(w * ratio)\n",
    "        input_image = input_image.resize((h, w), Image.BICUBIC)\n",
    "        input_image = np.asarray(input_image)\n",
    "        # RGB -> BGR\n",
    "        input_image = input_image[:, :, [2, 1, 0]]\n",
    "        input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n",
    "        # preprocess, (-1, 1)\n",
    "        input_image = -1 + 2 * input_image \n",
    "        input_image = Variable(input_image, volatile=True).float()\n",
    "        # forward\n",
    "        output_image = model(input_image)\n",
    "        output_image = output_image[0]\n",
    "        # BGR -> RGB\n",
    "        output_image = output_image[[2, 1, 0], :, :]\n",
    "        # deprocess, (0, 1)\n",
    "        output_image = output_image.data.cpu().float() * 0.5 + 0.5\n",
    "        # save\n",
    "        array = output_image.permute(1,2,0).numpy()\n",
    "        array = (255 * array).astype(np.uint8)\n",
    "\n",
    "        cv2.imshow('image', array)\n",
    "        \n",
    "    \n",
    "        #output_image_name = os.path.join('test_output1', str(frame_count) + '_' + 'Shinkai' + '.jpg')\n",
    "        #frame_count = frame_count+1\n",
    "        #vutils.save_image(output_image,output_image_name )\n",
    "\n",
    "\n",
    "        # Load the image from disk\n",
    "        #image = cv2.imread(output_image_name)\n",
    "\n",
    "        # Convert the image to a Mat object\n",
    "        #mat = cv2.UMat(image)\n",
    "\n",
    "        # Display the frame in a window\n",
    "        #cv2.imshow('image', mat)\n",
    "\n",
    "\n",
    "    # Wait for a key press to exit\n",
    "    if cv2.waitKey(1) == 27: # Esc key\n",
    "        break\n",
    "\n",
    "# Release the video capture device and close all windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fb51385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abstu\\AppData\\Local\\Temp\\ipykernel_8580\\4200631879.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  frame = Variable(frame, volatile=True).float()\n",
      "f:\\Work\\Anaconda\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "from network.Transformer import Transformer\n",
    "import time\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--input_dir', default = 'test_img')\n",
    "#parser.add_argument('--load_size', default = 450)\n",
    "#parser.add_argument('--model_path', default = './pretrained_model')\n",
    "#parser.add_argument('--style', default = 'Hayao')\n",
    "#parser.add_argument('--output_dir', default = 'test_output1')\n",
    "#parser.add_argument('--gpu', type=int, default = -1)\n",
    "\n",
    "#opt = parser.parse_args()\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = 'F:\\Work\\ML Projects\\CartoonGAN-Test-Pytorch-Torch\\Hayao_net_G_float.pth'\n",
    "\n",
    "Shinkai_net_G_float = torch.load(model_path)\n",
    "\n",
    "# load pretrained model\n",
    "model = Transformer()\n",
    "model.load_state_dict(Shinkai_net_G_float)\n",
    "model.eval()\n",
    "model.float()\n",
    "#print('Using', device)\n",
    "\n",
    "# Initialize the video capture device\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture the current frame\n",
    "    ret, frame = capture.read()\n",
    "    \n",
    "\n",
    "    # Check if the video capture was successful\n",
    "    if not ret:\n",
    "        print(\"Failed to capture video\")\n",
    "        break\n",
    "\n",
    "    # Save the frame as an image file\n",
    "    #filename = f\"frame{frame_count}.jpg\"\n",
    "    #cv2.imwrite(filename, frame)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        \n",
    "        # resize image, keep aspect ratio\n",
    "        # Resize the frame\n",
    "        frame = cv2.resize(frame, (450, 338))\n",
    "\n",
    "        # Convert the frame to a PyTorch tensor\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frame = transforms.ToTensor()(frame).unsqueeze(0)\n",
    "        frame = -1 + 2 * frame \n",
    "        frame = Variable(frame, volatile=True).float()\n",
    "        \n",
    "        # forward\n",
    "        output_image = model(frame)\n",
    "        output_image = output_image[0]\n",
    "        \n",
    "        # BGR -> RGB\n",
    "        output_image = output_image[[2, 1, 0], :, :]\n",
    "        \n",
    "        # deprocess, (0, 1)\n",
    "        output_image = output_image.data.cpu().float() * 0.5 + 0.5\n",
    "        \n",
    "        # save\n",
    "        array = output_image.permute(1, 2, 0).numpy()\n",
    "        array = (255 * array).astype(np.uint8)\n",
    "\n",
    "        cv2.imshow('image', array)\n",
    "\n",
    "    # Wait for a key press to exit\n",
    "    if cv2.waitKey(1) == 27: # Esc key\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae660f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.6788e-06, -4.2819e-06, -4.7644e-06,  ..., -9.2272e-06,\n",
       "           -8.3829e-06, -7.9004e-06],\n",
       "          [-3.7994e-06, -4.1613e-06, -4.7644e-06,  ..., -8.1417e-06,\n",
       "           -7.6592e-06, -7.4180e-06],\n",
       "          [-3.9201e-06, -4.0407e-06, -4.5231e-06,  ..., -7.6592e-06,\n",
       "           -7.2973e-06, -7.0561e-06],\n",
       "          ...,\n",
       "          [ 1.7490e-06,  1.9902e-06,  2.1108e-06,  ..., -1.3087e-05,\n",
       "           -1.3087e-05, -1.2846e-05],\n",
       "          [ 1.8696e-06,  1.9902e-06,  1.9902e-06,  ..., -1.3087e-05,\n",
       "           -1.3087e-05, -1.2846e-05],\n",
       "          [ 2.2314e-06,  2.3520e-06,  2.3520e-06,  ..., -1.3087e-05,\n",
       "           -1.3087e-05, -1.2846e-05]],\n",
       "\n",
       "         [[-5.0056e-06, -5.3675e-06, -5.4881e-06,  ..., -8.2623e-06,\n",
       "           -8.1417e-06, -7.9004e-06],\n",
       "          [-5.2469e-06, -5.3675e-06, -5.6087e-06,  ..., -8.1417e-06,\n",
       "           -8.0210e-06, -8.0210e-06],\n",
       "          [-5.3675e-06, -5.3675e-06, -5.4881e-06,  ..., -8.1417e-06,\n",
       "           -8.0210e-06, -8.0210e-06],\n",
       "          ...,\n",
       "          [ 1.3871e-06,  1.8696e-06,  1.9902e-06,  ..., -1.2846e-05,\n",
       "           -1.2846e-05, -1.2605e-05],\n",
       "          [ 1.3871e-06,  1.6283e-06,  1.7490e-06,  ..., -1.2846e-05,\n",
       "           -1.2846e-05, -1.2605e-05],\n",
       "          [ 1.3871e-06,  1.7490e-06,  1.8696e-06,  ..., -1.2725e-05,\n",
       "           -1.2725e-05, -1.2605e-05]],\n",
       "\n",
       "         [[-4.4025e-06, -5.0056e-06, -5.3675e-06,  ..., -8.8654e-06,\n",
       "           -8.5035e-06, -8.1417e-06],\n",
       "          [-4.6438e-06, -5.0056e-06, -5.3675e-06,  ..., -8.2623e-06,\n",
       "           -8.0210e-06, -7.9004e-06],\n",
       "          [-4.7644e-06, -5.0056e-06, -5.2469e-06,  ..., -8.0210e-06,\n",
       "           -7.9004e-06, -7.7798e-06],\n",
       "          ...,\n",
       "          [ 1.9902e-06,  2.3520e-06,  2.8345e-06,  ..., -1.2122e-05,\n",
       "           -1.2122e-05, -1.1881e-05],\n",
       "          [ 1.9902e-06,  2.2314e-06,  2.4727e-06,  ..., -1.2122e-05,\n",
       "           -1.2122e-05, -1.1881e-05],\n",
       "          [ 2.1108e-06,  2.3520e-06,  2.4727e-06,  ..., -1.2001e-05,\n",
       "           -1.2001e-05, -1.1881e-05]]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce1e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Release the video capture device and close all windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5577c0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(340, 452, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea975fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = (255 * array).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c11311f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(340, 452, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5750a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', array)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d5b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e165b89cf641f5f94a2ea239a4b88e5e1f4d1db2e9a896c3fd4c26211c89f4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
